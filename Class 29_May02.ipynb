{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dimensionality Reduction\n",
    "    - PCA (Principal Component Analysis) (Eigen values and Eigen vectors) - Unsupervised ML\n",
    "    - LDA (Linear Discriminant Analysis) - Supervised ML\n",
    "    \n",
    "- NLP\n",
    "    - NAIVE Bayes(Bayes Theorem)\n",
    "    - Latent Semantic Indexing(LSI)\n",
    "    - LDA (Latent Dirchlent Analysis)\n",
    "    \n",
    "- Speech to text\n",
    "    - pip install google-cloud-speech\n",
    "\n",
    "- Text pre-processing\n",
    "    - Convert to lower case\n",
    "    - Garbage value removal\n",
    "    - Stop words removal\n",
    "    - Stemming\n",
    "    - Lemmatization\n",
    "    - DTM\n",
    "        - TF IDF Vector\n",
    "        - Count vector\n",
    "- EDA\n",
    "    - Wordcounts\n",
    "    - Frequency tables\n",
    "    - Probability graphs\n",
    "    - Words combinations    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes' Theorem\n",
    "\n",
    "P(A/B) = P(A and B) / P(B)\n",
    "\n",
    "P(B/A) = P(B and A) / P(A)\n",
    "\n",
    "P(A and B) = P(B and A) \n",
    "\n",
    "P(A/B) * P(B) = P(B/A) * P(A)\n",
    "\n",
    "(P(A/B) * P(B)) * 1/P(B) = (P(B/A) * P(A)) * 1/P(B)\n",
    "\n",
    "P(A/B) = (P(B/A) * P(A)) / P(B)\n",
    "\n",
    "Probability of A , given that B has already occurred is equal to Probability of B, given that A has already occurred times Probability of A .. divided by Probability of B\n",
    "\n",
    "<img src='img/bayes-01.png'/>\n",
    "\n",
    "<b>Posterior Probability & Prior Probability</b>\n",
    "\n",
    "A posterior probability, in Bayesian statistics, is the revised or updated probability of an event occurring after taking into consideration new information. The posterior probability is calculated by updating the prior probability using Bayes' theorem. In statistical terms, the posterior probability is the probability of event A occurring given that event B has occurred.\n",
    "\n",
    "<b>What Does a Posterior Probability Tell You?</b>\n",
    "\n",
    "Bayes' theorem can be used in many applications, such as medicine, finance, and economics. In finance, Bayes' theorem can be used to update a previous belief once new information is obtained. Prior probability represents what is originally believed before new evidence is introduced, and posterior probability takes this new information into account.\n",
    "\n",
    "Posterior probability distributions should be a better reflection of the underlying truth of a data generating process than the prior probability since the posterior included more information. A posterior probability can subsequently become a prior for a new updated posterior probability as new information arises and is incorporated into the analysis.\n",
    "Example of a Posterior Probability\n",
    "\n",
    "As a simple example to envision posterior probability, suppose there are three acres of land with labels A, B and C. One acre has reserves of oil below its surface, while the other two do not. The prior probability of oil in acre C is one-third or 33%. A drilling test is conducted on acre B, and the results indicate that no oil is present at the location. With acre B eliminated, the posterior probability of acre C containing oil becomes 0.5, or 50%.\n",
    "\n",
    "<b>What Is Prior Probability?</b>\n",
    "\n",
    "Prior probability, in Bayesian statistical inference, is the probability of an event before new data is collected. This is the best rational assessment of the probability of an outcome based on the current knowledge before an experiment is performed.\n",
    "\n",
    "<b>Prior Probability Explained</b>\n",
    "\n",
    "The prior probability of an event will be revised as new data or information becomes available, to produce a more accurate measure of a potential outcome. That revised probability becomes the posterior probability and is calculated using Bayes' theorem. In statistical terms, the posterior probability is the probability of event A occurring given that event B has occurred.\n",
    "\n",
    "For example, three acres of land have the labels A, B, and C. One acre has reserves of oil below its surface, while the other two do not. The prior probability of oil being found on acre C is one third, or 0.333. But if a drilling test is conducted on acre B, and the results indicate that no oil is present at the location, then the posterior probability of oil being found on acres A and C become 0.5, as each acre has one out of two chances.\n",
    "\n",
    "Baye’s theorem is a very common and fundamental theorem used in data mining and machine learning.\n",
    "\n",
    "If we are interested in the probability of an event of which we have prior observations; we call this the prior probability. We'll deem this event A, and its probability P(A). If there is a second event that affects P(A), which we'll call event B, then we want to know what the probability of A is given B has occurred. In probabilistic notation, this is P(A|B), and is known as posterior probability or revised probability. This is because it has occurred after the original event, hence the post in posterior. This is how Baye’s theorem uniquely allows us to update our previous beliefs with new information.\n",
    "\n",
    "\n",
    "- sklearn.naive_bayes.MultinomialNB\n",
    "\n",
    "The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.\n",
    "\n",
    "- sklearn.naive_bayes.BernoulliNB\n",
    "\n",
    "Naive Bayes classifier for multivariate Bernoulli models.\n",
    "\n",
    "Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.\n",
    "\n",
    "- sklearn.naive_bayes.GaussianNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1320"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"D:\\sanooj\\datascience\\data\\amazon-reviews-unlocked-mobile-phones\\Amazon_Unlocked_Mobile.csv\")\n",
    "\n",
    "df = df.sample(100)\n",
    "df['Reviews'] = df['Reviews'].str.lower()\n",
    "\n",
    "unique_words = []\n",
    "for i in df['Reviews']:\n",
    "    for j in i.split():\n",
    "        if j not in unique_words:\n",
    "            unique_words.append(j)\n",
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>it's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>phone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>excellent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1315</td>\n",
       "      <td>you.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1316</td>\n",
       "      <td>damaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1317</td>\n",
       "      <td>china</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1318</td>\n",
       "      <td>aways</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1319</td>\n",
       "      <td>disconnect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1320 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "0           it's\n",
       "1              a\n",
       "2           good\n",
       "3          phone\n",
       "4      excellent\n",
       "...          ...\n",
       "1315       you.\"\n",
       "1316     damaged\n",
       "1317       china\n",
       "1318       aways\n",
       "1319  disconnect\n",
       "\n",
       "[1320 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(unique_words)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program -> program\n",
      "programming -> program\n",
      "programmer -> programm\n",
      "programs -> program\n",
      "programmers -> programm\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "words = ['program','programming','programmer','programs','programmers']\n",
    "\n",
    "for word in words:\n",
    "    print(word + ' -> ' +ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Rani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks ->  rock\n",
      "good ->  good\n",
      "better ->  better\n",
      "corpora ->  corpus\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print('rocks -> ',lemmatizer.lemmatize('rocks'))\n",
    "print('good -> ',lemmatizer.lemmatize('good'))\n",
    "\n",
    "print('better -> ',lemmatizer.lemmatize('better'))\n",
    "print('corpora -> ',lemmatizer.lemmatize('corpora'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
