{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity='all'\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms (Clustering - Unsupervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In unsupervised algorithms there is a no 'y' variable\n",
    "# A good cluster is such that,\n",
    "# 1) The distance between all the points in the cluster to it's centroid is minimum\n",
    "# 2) The distance between different clusters formed should be maximum\n",
    "\n",
    "# Different ways of finding distances are,\n",
    "# 1) Manhattan distance\n",
    "# 2) Minkowski distance\n",
    "# 3) Euclidean distance\n",
    "\n",
    "# Entropy is a measure of how homegenous or heterogenous the data is, within a cluster\n",
    "# Lesser the entropy better the cluster\n",
    "# For homogenous data, entropy will be less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different clustering algorithms,\n",
    "# 1) K-Means\n",
    "# 2) K-Modes\n",
    "# 3) Agglomerative\n",
    "\n",
    "# When we group the data by rows then its called CLUSTERING or SEGMENTATION\n",
    "# When we group the data by columns then its called DIMENSIONALITY REDUCTION or FACTOR ANALYSIS\n",
    "\n",
    "# Steps involved in implementing cluster analysis in real time\n",
    "# 1) Identify the clusters\n",
    "# 2) Study the behaviour of the clusters\n",
    "# 3) Design the strategies according to the cluster behaviour\n",
    "\n",
    "# What algorithms to use when ?\n",
    "# K-Means - When all the data is continous variables\n",
    "# K-Modes - When all the data is class variables\n",
    "# K-Prototypes - When the data is a mix of continous and class variables\n",
    "\n",
    "\n",
    "# In all clustering algorithms, if the distance between the data row and random point is same, then assign the \n",
    "# point to the cluster with least(minimum) variance\n",
    "\n",
    "# After forming clusters, target the members within the cluster. e.g. find all rows within cluster that has \n",
    "# less mean value than the mean value of the cluster and start targetting such users to improve sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to choose the columns to be considered for K-Means ?\n",
    "# Pick continous variables \n",
    "# One way is to check how many unique values for a column\n",
    "# A better way is to check the variance of the fields. Higher the variance we need to consider such columns\n",
    "# Once the cluster is formed plot it and see how it looks. Also look min/max/mean etc to see if the \n",
    "# clusters are formed without any overlaps\n",
    "\n",
    "# Steps\n",
    "# 1) Let's identify the number of clusters. There is a statistical way to find the number of clusters. For now,\n",
    "#    let's assume the number of clusters to be 3\n",
    "# 2) Pick 3 random rows from the data set ( 3 because we are going to create 3 clusters)\n",
    "# 3) Find the distance of all the rows with these 3 random points. Each row will be associated with 3  \n",
    "#    distances. Row will be assigned to the cluster with least distance. Once we do this for all the rows \n",
    "#    in the dataset, we would have assigned the row to either cluster1, cluster2 or cluster3\n",
    "# 4) Find the centroid of each cluster. Centroid will be calculated by finding the mean for each column\n",
    "#    i.e centroid = (mean of column1, mean of column2, mean of column3 ....). Since we have 3 clusters, there \n",
    "#    will be 3 centroids now\n",
    "# 5) Repeat step 2 - 4 by making the centroids as the new random points. After one such iteration the cluster\n",
    "#    would've changed\n",
    "# 6) In the above step, if the cluster doesn't change between 'n' th and (n+1) th iteration, then by now we \n",
    "#    would've formed the final clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above step is very similar to KNN, just that in K-Means there is no 'y' variable but in KNN there is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The statistical way of finding cluster is called K-Elbow\n",
    "# What is K-Elbow ?\n",
    "# Divide the entire dataset into 1 cluster, find the variance\n",
    "# Divide the entire dataset into 2 clusters, find the variance of each cluster and sum it\n",
    "# Divide the entire dataset into 3 clusters, find the variance of each cluster and sum it \n",
    "# and so on...\n",
    "# After a particular point the sum of variance doesn't vary much. The point where we see a sharp drop in the \n",
    "# sum of variance we can conside the number of cluster at that point as the number of clusters.\n",
    "\n",
    "# If we plot the number of clusters in x axis and the sum of variance in y axis , then we can see a 'Elbow' \n",
    "# like shape when the sum of variance drops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps\n",
    "# 1) Let's identify the number of clusters. There is a way to find the number of clusters, yet to be covered. \n",
    "#    The method name is 'Silhouette'\n",
    "# 2) Pick 3 random rows from the data set ( 3 because we are going to create 3 clusters)\n",
    "# 3) Form dissimilarity index(put value of 1 if not a match and 0 if its a match) for each rows and for \n",
    "#    each random points. i.e. Compare column1 value of the row with the column1 value of the random point and \n",
    "#    then put value of 1 and 0.  lower the sum of the elements in the dissimilarity index , \n",
    "#    closer the corresponding row to the respective random point.\n",
    "# 4) After 1 iteration we would've assigned the initial cluster. Find the centroid of each cluster. \n",
    "#    Centroid will be calculated by finding the mode for each column i.e centroid = (mode of column1,\n",
    "#    mode of column2, mode of column3 ....). Since we have 3 clusters, there \n",
    "#    will be 3 centroids now\n",
    "# 5) Repeat step 2 - 4 by making the centroids as the new random points. After one such iteration the cluster\n",
    "#    would've changed\n",
    "# 6) In the above step, if the cluster doesn't change between 'n' th and (n+1) th iteration, then by now we \n",
    "#    would've formed the final clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is not a popular clustering method because of the heavy number of computations involved\n",
    "# if there is a 100 row data set, then we form a 100*100 matrix. \n",
    "# Positive is that the visualization created is very self explanatory\n",
    "\n",
    "# 5 types of link functions\n",
    "# 1) Single link (also known as Min link)\n",
    "# 2) Complete link\n",
    "# 3) Average Link\n",
    "# 4) Centroids\n",
    "# 5) Wards method (also known as Min Variance)\n",
    "\n",
    "# For each data rows, r1,r2,r3....,rn find the distances between each row and every other row. This is where \n",
    "# a n*n matrix will be formed\n",
    "#      r1   r2   r3   r4\n",
    "# r1   0    d1   d2   d3\n",
    "# r2   d1   0    d4   d5  \n",
    "# r3   d2   d4   0    d6          \n",
    "# r4   d3   d5   d6   0         \n",
    "#\n",
    "# In this matrix we have distances between every row between every other row\n",
    "# find the minimum distance , let's assumg d5 is minimum. Then in the next iteration we consider r4,r2 as \n",
    "# combined data point\n",
    "#        r1   r2   r3   r2r4\n",
    "# r1     0    d1   d2   d3\n",
    "# r2r4   d1   0    d4   d5  \n",
    "# r3     d2   d4   0    d6          \n",
    "# \n",
    "# We have to repeat the above steps till we get to 1 cluster (not too clear on the intend behind this)\n",
    "# when we find the distance between r1 and r2r4 i.e. r1 - r2r4\n",
    "# 1) if we consider the minimum value between (r1-r2) AND (r1-r4) then that's called Single link\n",
    "# 2) if we consider the maximum value between (r1-r2) AND (r1-r4) then that's called Complete link\n",
    "# 3) if we consider the average of (r1-r2) AND (r1-r4) then that's called Average Link\n",
    "# 4) if we find the centroid of (r1) and (r2 r4) and if we consider the distance between the centroids then it's \n",
    "#    called Centroids Link\n",
    "# 5) Ward's Link method \n",
    "#    Find the mean of each cluster.\n",
    "#    Calculate the distance between each object in a particular cluster, and that clusterâ€™s mean.\n",
    "#    Square the differences from Step 2.\n",
    "#    Sum (add up) the squared values from Step 3.\n",
    "#    Add up all the sums of squares from Step 4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dendogram is the visual representation of how clusters are formed in an agglomerative clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
