{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Algorithms\n",
    "\n",
    "Last class we saw that we can employee different techniques for improving the model accuracy (e.g. blending , boosting etc). We keep track of the model metrics (accuracy, tn, fn , etc etc) for each model and pick the one that's performing the best. For competitions, however this will not be good enough. We have to see approaches like blending to make a better algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose best hyperparameters with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "params_grid = { 'max_depth' : [3,None],\n",
    "\t\t'min_samples_split' : [2,3,10],\n",
    "\t\t'min_samples_leaf':[1,3,10],\n",
    "\t\t'bootstrap':[True,False]\n",
    "\t\t'criterion':['gini','entropy']}\n",
    "\n",
    "grid_search = GridSearchCV(rf_clf,params_grid,n_jobs=-1,cv=5,verbose=1,scoring='accuracy')\n",
    "\n",
    "grid_search.fit(X_train,y_train)\n",
    "\n",
    "grid_search.best_score_\n",
    "\n",
    "grid_search.best_estimator_.get_params()\n",
    "\n",
    "print_score(grid_search, X_train, y_train, X_test, y_test, train=True)\n",
    "\n",
    "print_score(grid_search, X_train, y_train, X_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Fold Validation\n",
    "\n",
    "Run cross fold validation and see how model is performing for each data splits. If the model is performing with more or less same level of accuracies across multiple splits, then that's a good model\n",
    "\n",
    "See the below example, cv = 10 for a datasize of 500 records means each split has 50 records and we run the train on 9 samples and test on remaining 1 sample and so on for each split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(rfc,x,y,cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blending\n",
    "\n",
    "There are different types of blending. One of them is explained in the previous class. The other type is explained below,\n",
    "\n",
    "- We train the model on 'n' algorithms\n",
    "- We take the average predictions of each algorithm and give that as the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model Building\n",
    "    - Machine Learning Models\n",
    "    - Accuracy Score and hyper-parameter tuning\n",
    "    - Single model which give you highest accuracy is chosen\n",
    "    - deploy and we use this for prediction\n",
    "\n",
    "- Advanced users/kaggle competition\n",
    "    - Stacking models\n",
    "    - boosting\n",
    "    - use this models for predicting the feature observations\n",
    "    - trade-off between interpretability - accuracy (complex techniques such as stacking gives better accuracy but interpretability is lost)\n",
    "    - gridsearch\n",
    "    - Find optimized parameters for each algorithms\n",
    "    - stacking or boosting algorithms\n",
    "    - check for accuracy\n",
    "    - k-fold validation\n",
    "    - submit prediction once you have k fold validation values in similar ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "While we develop a model we have to select the features(predictors) that helps the most in predicting/classifying the y-variable. The technique to achieve this is called 'feature engineering'. We try to select the most important features and as well try to see if we can make the insignificant features as significant by applying some techniques\n",
    "\n",
    "\n",
    "- Feature Engineering\n",
    "- Forward Selection\n",
    "- Backward Elimination\n",
    "- Stepwise Selection\n",
    "- Lowvariance Method\n",
    "- PCA (Principal Component Analysis)\n",
    "- LDA (Linear Discriminant Analysis)\n",
    "- TSNE (T Student Neighbourhood Embedding)\n",
    "\n",
    "Objective - \n",
    "Identify the imporant columns using \n",
    "- feature selection\n",
    "- backward elimination\n",
    "- stepwise selection\n",
    "- lowvariance method\n",
    "\n",
    "Convert insignificant to significant\n",
    "- PCA (Principal Component Analysis)\n",
    "- LDA (Linear Discriminant Analysis)\n",
    "- TSNE (T Student Neighbour Embedding)\n",
    "- Transformations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
