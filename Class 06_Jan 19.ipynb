{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Day 6 - Class </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "\n",
    "Machine learning is an application of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Machine learning focuses on the development of computer programs that can access data and use it learn for themselves.\n",
    "\n",
    "Machine learning algorithms can be classified into,\n",
    "- Supervised machine learning algorithms\n",
    "- Unsupervised machine learning algorithms\n",
    "- Semi-supervised machine learning algorithms\n",
    "- Reinforcement machine learning algorithms\n",
    "\n",
    "### Supervised learning\n",
    "Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs.It infers a function from labeled training data consisting of a set of training examples.In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value.  A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a \"reasonable\" way.\n",
    "\n",
    "Here we have a y variable that's depending on n number of independent variables x1,x2,x3,.....,xn\n",
    "y = f(x1,x2,x3,....xn). If we have labelled data i.e. for a given set of x variables if the value of y is provided, then its a supervised learning problem.\n",
    "\n",
    "If we have a finite amount of unique values in 'y' variable then it becomes <b>classification</b> problem. i.e. when you predict a discrete variable then it's a classification problem\n",
    "If we have infinite amount of unique values in 'y' variable then it becomes <b>regression</b> problem i.e. when you predict a continous variable then it's a regression problem\n",
    "\n",
    "\n",
    "### Unsupervised learning\n",
    "Unsupervised learning is a type of machine learning that looks for previously undetected patterns in a data set with no pre-existing labels and with a minimum of human supervision. Two of the main methods used in unsupervised learning are principal component and cluster analysis.\n",
    "\n",
    "In this case, there is no 'y' variable or labelled data.\n",
    "\n",
    "When we do row level study then it becomes clustering/segmentation/grouping problem\n",
    "When we do column level study then it becomes dimensionality reduction/factor analysis\n",
    "\n",
    "### Semi-supervised learning\n",
    "Semi-supervised learning is an approach to machine learning that combines a small amount of labeled data with a large amount of unlabeled data during training. Semi-supervised learning falls between unsupervised learning (with no labeled training data) and supervised learning (with only labeled training data). \n",
    "\n",
    "### Reinforcement learning\n",
    "Reinforcement learning (RL) is an area of machine learning concerned with how software agents ought to take actions in an environment in order to maximize the notion of cumulative reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Types\n",
    "- Numeric\n",
    "    - Continous(infinite values)\n",
    "    - Discrete(finite values)\n",
    "- Categorical\n",
    "    - Ordinal(When there is order e.g. rank, rating)\n",
    "    - Nominal(When there is no order e.g. gender, boolean)\n",
    "    \n",
    "When working with statistics, it’s important to recognize the different types of data: numerical (discrete and continuous), categorical, and ordinal. \n",
    "\n",
    "Most data fall into one of two groups: numerical or categorical.\n",
    "\n",
    "### Numerical data\n",
    "Numerical data. These data have meaning as a measurement, such as a person’s height, weight, IQ, or blood pressure; or they’re a count, such as the number of stock shares a person owns, how many teeth a dog has, or how many pages you can read of your favorite book before you fall asleep. (Statisticians also call numerical data quantitative data.)\n",
    "\n",
    "Numerical data can be further broken into two types: discrete and continuous.\n",
    "\n",
    "#### Discrete\n",
    "Discrete data represent items that can be counted; they take on possible values that can be listed out. The list of possible values may be fixed (also called finite); or it may go from 0, 1, 2, on to infinity (making it countably infinite). For example, the number of heads in 100 coin flips takes on values from 0 through 100 (finite case), but the number of flips needed to get 100 heads takes on values from 100 (the fastest scenario) on up to infinity (if you never get to that 100th heads). Its possible values are listed as 100, 101, 102, 103, . . . (representing the countably infinite case).\n",
    "\n",
    "#### Continous\n",
    "Continuous data represent measurements; their possible values cannot be counted and can only be described using intervals on the real number line. For example, the exact amount of gas purchased at the pump for cars with 20-gallon tanks would be continuous data from 0 gallons to 20 gallons, represented by the interval [0, 20], inclusive. You might pump 8.40 gallons, or 8.41, or 8.414863 gallons, or any possible number from 0 to 20. In this way, continuous data can be thought of as being uncountably infinite.\n",
    "\n",
    "### Categorical data\n",
    "Categorical data: Categorical data represent characteristics such as a person’s gender, marital status, hometown, or the types of movies they like. Categorical data can take on numerical values (such as “1” indicating male and “2” indicating female), but those numbers don’t have mathematical meaning. You couldn’t add them together, for example. (Other names for categorical data are qualitative data, or Yes/No data.)\n",
    "\n",
    "#### Ordinal\n",
    "Ordinal data mixes numerical and categorical data. The data fall into categories, but the numbers placed on the categories have meaning. For example, rating a restaurant on a scale from 0 (lowest) to 4 (highest) stars gives ordinal data. Ordinal data are often treated as categorical, where the groups are ordered when graphs and charts are made. However, unlike categorical data, the numbers do have mathematical meaning. For example, if you survey 100 people and ask them to rate a restaurant on a scale from 0 to 4, taking the average of the 100 responses will have meaning. This would not be the case with categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics Types\n",
    "Descriptive and inferential statistics are two broad categories in the field of statistics.Interestingly, some of the statistical measures are similar, but the goals and methodologies are very different.\n",
    "- Descriptive statistics \n",
    "- Inferential statistics\n",
    "When analysing data, such as the marks achieved by 100 students for a piece of coursework, it is possible to use both descriptive and inferential statistics in your analysis of their marks. Typically, in most research conducted on groups of people, you will use both descriptive and inferential statistics to analyse your results and draw conclusions.\n",
    "\n",
    "### Descriptive statistics\n",
    "Descriptive statistics is the term given to the analysis of data that helps describe, show or summarize data in a meaningful way such that, for example, patterns might emerge from the data. Descriptive statistics do not, however, allow us to make conclusions beyond the data we have analysed or reach conclusions regarding any hypotheses we might have made. They are simply a way to describe our data.\n",
    "\n",
    "Descriptive statistics are very important because if we simply presented our raw data it would be hard to visualize what the data was showing, especially if there was a lot of it. Descriptive statistics therefore enables us to present the data in a more meaningful way, which allows simpler interpretation of the data\n",
    "\n",
    "Descriptive statistics describe a sample. That’s pretty straightforward. You simply take a group that you’re interested in, record data about the group members, and then use summary statistics and graphs to present the group properties. With descriptive statistics, there is no uncertainty because you are describing only the people or items that you actually measure. You’re not trying to infer properties about a larger population.\n",
    "\n",
    "#### Common tools of descriptive statistics\n",
    "<b>Central tendency</b>: Use the mean or the median to locate the center of the dataset. This measure tells you where most values fall.\n",
    "\n",
    "<b>Dispersion</b>: How far out from the center do the data extend? You can use the range or standard deviation to measure the dispersion. A low dispersion indicates that the values cluster more tightly around the center. Higher dispersion signifies that data points fall further away from the center. We can also graph the frequency distribution. To describe this spread, a number of statistics are available to us, including the range, quartiles, absolute deviation, variance and standard deviation.\n",
    "\n",
    "<b>Skewness</b>: The measure tells you whether the distribution of values is symmetric or skewed.\n",
    "\n",
    "<img src='img/skew-01.png' />\n",
    "\n",
    "\n",
    "An example:\n",
    "\n",
    "<img src='img/descriptive-statistics-01.png' />\n",
    "\n",
    "#### What are the limitations of descriptive statistics?\n",
    "\n",
    "Descriptive statistics are limited in so much that they only allow you to make summations about the people or objects that you have actually measured. You cannot use the data you have collected to generalize to other people or objects (i.e., using data from a sample to infer the properties/parameters of a population). For example, if you tested a drug to beat cancer and it worked in your patients, you cannot claim that it would work in other cancer patients only relying on descriptive statistics (but inferential statistics would give you this opportunity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferential Statistics\n",
    "More often than not, you do not have access to the whole population you are interested in investigating, but only a limited number of data instead. For example, you might be interested in the exam marks of all students in the UK. It is not feasible to measure all exam marks of all students in the whole of the UK so you have to measure a smaller sample of students (e.g., 100 students), which are used to represent the larger population of all UK students. Properties of samples, such as the mean or standard deviation, are not called parameters, but statistics. Inferential statistics are techniques that allow us to use these samples to make generalizations about the populations from which the samples were drawn. It is, therefore, important that the sample accurately represents the population. The process of achieving this is called sampling.\n",
    "\n",
    "#### Standard analysis tools of inferential statistics\n",
    "The most common methodologies in inferential statistics are hypothesis tests, confidence intervals, and regression analysis. Also, Analysis of variance can be another method.\n",
    "\n",
    "<b>Hypothesis tests</b>\n",
    "\n",
    "Hypothesis tests use sample data answer questions like the following:\n",
    "\n",
    "- Is the population mean greater than or less than a particular value?\n",
    "- Are the means of two or more populations different from each other?\n",
    "\n",
    "For example, if we study the effectiveness of a new medication by comparing the outcomes in a treatment and control group, hypothesis tests can tell us whether the drug’s effect that we observe in the sample is likely to exist in the population. After all, we don’t want to use the medication if it is effective only in our specific sample. Instead, we need evidence that it’ll be useful in the entire population of patients. Hypothesis tests allow us to draw these types of conclusions about entire populations.\n",
    "\n",
    "\n",
    "<b>Confidence intervals (CIs)</b>\n",
    "\n",
    "In inferential statistics, a primary goal is to estimate population parameters. These parameters are the unknown values for the entire population, such as the population mean and standard deviation. These parameter values are not only unknown but almost always unknowable. Typically, it’s impossible to measure an entire population. The sampling error I mentioned earlier produces uncertainty, or a margin of error, around our estimates.\n",
    "\n",
    "Suppose we define our population as all high school basketball players. Then, we draw a random sample from this population and calculate the mean height of 181 cm. This sample estimate of 181 cm is the best estimate of the mean height of the population. However, it’s virtually guaranteed that our estimate of the population parameter is not exactly correct.\n",
    "\n",
    "Confidence intervals incorporate the uncertainty and sample error to create a range of values the actual population value is like to fall within. For example, a confidence interval of [176 186] indicates that we can be confident that the real population mean falls within this range.\n",
    "\n",
    "An example\n",
    "\n",
    "Suppose we conducted our study on test scores for a specific class as I detailed in the descriptive statistics section. Now we want to perform an inferential statistics study for that same test. Let’s assume it is a standardized statewide test. By using the same test, but now with the goal of drawing inferences about a population, I can show you how that changes the way we conduct the study and the results that we present.\n",
    "\n",
    "In descriptive statistics, we picked the specific class that we wanted to describe and recorded all of the test scores for that class. Nice and simple. For inferential statistics, we need to define the population and then draw a random sample from that population.\n",
    "\n",
    "Let’s define our population as 8th-grade students in public schools in the State of Pennsylvania in the United States. We need to devise a random sampling plan to help ensure a representative sample. This process can actually be arduous. For the sake of this example, assume that we are provided a list of names for the entire population and draw a random sample of 100 students from it and obtain their test scores. Note that these students will not be in one class, but from many different classes in different schools across the state.\n",
    "Inferential statistics results\n",
    "\n",
    "For inferential statistics, we can calculate the point estimate for the mean, standard deviation, and proportion for our random sample. However, it is staggeringly improbable that any of these point estimates are exactly correct, and there is no way to know for sure anyway. Because we can’t measure all subjects in this population, there is a margin of error around these statistics. Consequently, I’ll report the confidence intervals for the mean, standard deviation, and the proportion of satisfactory scores (>=70). Here is the CSV data file: Inferential_statistics.\n",
    "\n",
    "<img src='img/inferential-statistics-01.png' />\n",
    "\n",
    "#### What are the limitations of inferential statistics?\n",
    "\n",
    "There are two main limitations to the use of inferential statistics. The first, and most important limitation, which is present in all inferential statistics, is that you are providing data about a population that you have not fully measured, and therefore, cannot ever be completely sure that the values/statistics you calculate are correct. Remember, inferential statistics are based on the concept of using the values measured in a sample to estimate/infer the values that would be measured in a population; there will always be a degree of uncertainty in doing this. The second limitation is connected with the first limitation. Some, but not all, inferential tests require the user (i.e., you) to make educated guesses (based on theory) to run the inferential tests. Again, there will be some uncertainty in this process, which will have repercussions on the certainty of the results of some inferential statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean\n",
    "Various types of mean are,\n",
    "- Root mean square\n",
    "- Arithmetic mean\n",
    "- Geometric mean\n",
    "- Harmonic mean\n",
    "\n",
    "For positive numbers a & b,\n",
    "RMS ≥ AM ≥ GM ≥ HM\n",
    "\n",
    "### Root mean square\n",
    "Root mean square is also known as quadratic mean. Used in both, statistics and mathematics, this formula gives the total sum of square root of each data in an observation. It is generally denoted by Xrms.\n",
    "\n",
    "The formula of root mean square is:\n",
    "\n",
    "<img src='img/rms-01.png'/>\n",
    "\n",
    "### Arithmetic mean\n",
    "The arithmetic mean is the simplest and most widely used measure of a mean, or average. It simply involves taking the sum of a group of numbers, then dividing that sum by the count of the numbers used in the series.\n",
    "<img src='img/am-01.png'/>\n",
    "\n",
    "### Geometric mean\n",
    "A geometric mean is often used when comparing different items when each item has multiple properties that have different numeric ranges.\n",
    "\n",
    "For example, the geometric mean can give a meaningful value to compare two companies which are each rated at \n",
    "\n",
    "0 to 5 for their environmental sustainability, and are rated at \n",
    "\n",
    "0 to 100 for their financial viability. \n",
    "\n",
    "If an arithmetic mean were used instead of a geometric mean, the financial viability would have greater weight because its numeric range is larger. That is, a small percentage change in the financial rating (e.g. going from 80 to 90) makes a much larger difference in the arithmetic mean than a large percentage change in environmental sustainability (e.g. going from 2 to 5). \n",
    "\n",
    "The use of a geometric mean normalizes the differently-ranged values, meaning a given percentage change in any of the properties has the same effect on the geometric mean. So, a 20% change in environmental sustainability from 4 to 4.8 has the same effect on the geometric mean as a 20% change in financial viability from 60 to 72.\n",
    "\n",
    "<img src='img/gm-02.png'/>\n",
    "\n",
    "### Harmonic mean\n",
    "Harmonic means are often used in averaging things like rates (e.g., the average travel speed given a duration of several trips).\n",
    "<img src='img/hm-01.png'/>\n",
    "\n",
    "Also note,\n",
    "<img src='img/means-01.png'/>\n",
    "\n",
    "If the growth is exponential we will use GM (e.g. x^2 , x^3 etc)\n",
    "\n",
    "If the growth is up & down (e.g. sine wave) we use harmonic mean\n",
    "\n",
    "Everything else we use AM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Errors, Loss, Cost & Mean Absolute Error\n",
    "Let's say we have a set of numbers, \n",
    "- we find the AM, GM & HM of these numbers. i.e. we find predicted values\n",
    "- Find the error, i.e. how much actual value is deviating from the predicted values. Do this for AM, HM, GM\n",
    "- Error is also known as loss/deviation\n",
    "- The sum of all 'loss' or 'errors' is known as 'cost'\n",
    "- The average of 'cost' is known as 'Mean Absolute Error'. \n",
    "- 'Absolute Error' is the error without sign (get rid of -ve)\n",
    "\n",
    "The intention of any ML algorithm will be to reduce the cost function\n",
    "\n",
    "\n",
    "## Median\n",
    "The median is the middle number in a sorted, ascending or descending, list of numbers and can be more descriptive of that data set than the average. The median is sometimes used as opposed to the mean when there are outliers in the sequence that might skew the average of the values.\n",
    "\n",
    "<b> Mean is sensitive to outliers </b>. As you can imagine a huge outlier will influence the mean value.\n",
    "\n",
    "<b> Median is NOT sensitive to outliers </b>. When there are outliers then it's better to go for median.\n",
    "\n",
    "While dealing with continous variables, we can use median for null value treatment\n",
    "\n",
    "## Mode\n",
    "The number which appears most often in a set of numbers. Example: in {6, 3, 9, 6, 6, 5, 9, 3} the Mode is 6 (it occurs most often)\n",
    "\n",
    "While dealing with categorical variables, we can use mode for null value treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[66, 81, 85, 54, 84],\n",
       "       [72, 42, 54, 85,  8],\n",
       "       [28, 97, 43,  8, 25],\n",
       "       [10, 92, 51, 52, 19],\n",
       "       [13, 73, 88, 84, 25],\n",
       "       [62, 50, 17, 80, 56],\n",
       "       [ 8, 50, 39, 84, 42],\n",
       "       [58, 74, 12, 46, 46],\n",
       "       [61, 86, 70, 69, 25],\n",
       "       [86,  6, 42,  3, 81],\n",
       "       [67,  2, 32, 85, 65],\n",
       "       [77, 56, 56,  2, 56],\n",
       "       [36, 22, 74, 19, 43],\n",
       "       [61, 23, 19, 27, 99],\n",
       "       [58, 44, 32, 46, 32],\n",
       "       [84,  7, 46, 72, 29],\n",
       "       [59, 15, 50, 42, 17],\n",
       "       [74, 53, 13, 60, 31],\n",
       "       [76, 35, 26, 87,  2],\n",
       "       [23, 13, 69, 89,  8],\n",
       "       [81, 51,  4, 52, 91],\n",
       "       [79, 42, 85, 11, 93],\n",
       "       [27, 31, 53, 25, 16],\n",
       "       [ 4, 90, 78, 70, 54],\n",
       "       [38, 48, 50, 97,  5]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Mean\n",
    "import numpy as np\n",
    "marks = np.random.randint(1,100,(25,5))\n",
    "marks.shape\n",
    "marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52.32, 47.32, 47.52, 53.96, 42.08]\n",
      "[41.535804005220484, 34.94105662315994, 39.08107459108647, 38.90256160769999, 29.89321258007173]\n",
      "[26.18473266467462, 18.19685075180491, 27.11568734152948, 17.177510634210307, 16.271158523664155]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(5, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## come up with a 5*3 matrix that can hold the AM, GM, HM for each of the rows\n",
    "from scipy.stats import gmean,hmean\n",
    "\n",
    "arithmeticmeanarray = []\n",
    "geometricmeanarray = []\n",
    "harmonicmeanarray = []\n",
    "for i in range(0,5):\n",
    "    subjectmarks = marks[:,i]\n",
    "    arithmeticmeanarray.append(subjectmarks.mean())\n",
    "    geometricmeanarray.append(gmean(subjectmarks))\n",
    "    harmonicmeanarray.append(hmean(subjectmarks))\n",
    "\n",
    "print(arithmeticmeanarray)\n",
    "print(geometricmeanarray)\n",
    "print(harmonicmeanarray)\n",
    "\n",
    "combinedmean = np.array([arithmeticmeanarray,geometricmeanarray,harmonicmeanarray])\n",
    "combinedmean.shape\n",
    "combinedmeantranspose = combinedmean.T\n",
    "combinedmeantranspose.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IQR (Inter Quartile Range)\n",
    "In descriptive statistics, the interquartile range, also called the midspread, middle 50%, or H‑spread, is a measure of statistical dispersion, being equal to the difference between 75th and 25th percentiles, or between upper and lower quartiles, IQR = Q₃ − Q₁.\n",
    "\n",
    "The interquartile range (IQR) is a measure of variability, based on dividing a data set into quartiles. Quartiles divide a rank-ordered data set into four equal parts. The values that divide each part are called the first, second, and third quartiles; and they are denoted by Q1, Q2, and Q3, respectively.\n",
    "\n",
    "\n",
    "Quartile 1 or Q1 is the data at the 0.25th quartile\n",
    "\n",
    "Quartile 3 or Q3 is the data at the 0.75th quartile\n",
    "\n",
    "E.g. - Imagine a dataset of 200 rows,\n",
    "- Sort the entire data in ascending order\n",
    "- Take the data at the 50th position. This will be Q1\n",
    "- Take the data at the 150th position. This will be Q3\n",
    "\n",
    "IQR is the difference between Q3 and Q1, i.e. <b> IQR = Q3 - Q1 </b>\n",
    "\n",
    "Now outliers will be, \n",
    "\n",
    "UTV = Q3 + (1.5 * IQR)\n",
    "\n",
    "LTV = Q1 - (1.5 * IQR)\n",
    "\n",
    "IQR should be applied on normal data distribution only. We can do IQR on positively skewed or negatively skewed distributions as well, but first we will have to convert those skewed distributions to a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program for IQR & outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143.0 -41.0\n",
      "\n",
      "[[ 66  81  85  54  84]\n",
      " [500  42  54  85   8]\n",
      " [ 28  97  43   8  25]\n",
      " [ 10  92  51  52  19]\n",
      " [ 13  73  88  84  25]\n",
      " [ 62  50  17  80  56]\n",
      " [  8  50  39  84  42]\n",
      " [ 58  74  12  46  46]\n",
      " [ 61  86  70  69  25]\n",
      " [ 86   6  42   3  81]\n",
      " [ 67   2  32  85  65]\n",
      " [ 77  56  56   2  56]\n",
      " [ 36  22  74  19  43]\n",
      " [ 61  23  19  27  99]\n",
      " [ 58  44  32  46  32]\n",
      " [ 84   7  46  72  29]\n",
      " [ 59  15  50  42  17]\n",
      " [ 74  53  13  60  31]\n",
      " [ 76  35  26  87   2]\n",
      " [ 23  13  69  89   8]\n",
      " [ 81  51   4  52  91]\n",
      " [ 79  42  85  11  93]\n",
      " [ 27  31  53  25  16]\n",
      " [  4  90  78  70  54]\n",
      " [ 38  48  50  97   5]]\n",
      "\n",
      "outlier ->  500\n"
     ]
    }
   ],
   "source": [
    "q1 = np.quantile(marks[:,0],0.25)\n",
    "q3 = np.quantile(marks[:,0],0.75)\n",
    "\n",
    "iqr = q3 - q1\n",
    "\n",
    "## find outliers by taking value 1.5 times greater than IQR or 1.5 times lesser than IQR\n",
    "utv = q3 + (1.5*iqr)\n",
    "ltv = q1 - (1.5*iqr)\n",
    "# Any value greater than utv will be an outlier\n",
    "# Any value lesser than ltv will be an outlier\n",
    "print(utv,ltv)\n",
    "print()\n",
    "# let's introduce an outlier by putting 500 as the value in second row and first column\n",
    "marks[1,0] = 500\n",
    "print(marks)\n",
    "print()\n",
    "for i in marks[:,0]:\n",
    "    if((i> utv) or (i<ltv)):\n",
    "        print('outlier -> ', i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skewness\n",
    "\n",
    "The normal distribution is a continuous probability distribution that is symmetrical on both sides of the mean, so the right side of the center is a mirror image of the left side. \n",
    "\n",
    "The area under the normal distribution curve represents probability and the total area under the curve sums to one.\n",
    "\n",
    "Most of the continuous data values in a normal distribution tend to cluster around the mean, and the further a value is from the mean, the less likely it is to occur. The tails are asymptotic, which means that they approach but never quite meet the horizon (i.e. x-axis).\n",
    "\n",
    "For a perfectly normal distribution the mean, median and mode will be the same value, visually represented by the peak of the curve. \n",
    "\n",
    "<img src='img/skew-01.png'/>\n",
    "\n",
    "Consider the two distributions in the figure just below. Within each graph, the values on the right side of the distribution taper differently from the values on the left side. These tapering sides are called tails, and they provide a visual means to determine which of the two kinds of skewness a distribution has:\n",
    "\n",
    "<img src='img/skew-02.png'/>\n",
    "\n",
    "\n",
    "negative skew: The left tail is longer; the mass of the distribution is concentrated on the right of the figure. The distribution is said to be left-skewed, left-tailed, or skewed to the left, despite the fact that the curve itself appears to be skewed or leaning to the right; left instead refers to the left tail being drawn out and, often, the mean being skewed to the left of a typical center of the data. A left-skewed distribution usually appears as a right-leaning curve.\n",
    "\n",
    "positive skew: The right tail is longer; the mass of the distribution is concentrated on the left of the figure. The distribution is said to be right-skewed, right-tailed, or skewed to the right, despite the fact that the curve itself appears to be skewed or leaning to the left; right instead refers to the right tail being drawn out and, often, the mean being skewed to the right of a typical center of the data. A right-skewed distribution usually appears as a left-leaning curve.\n",
    "\n",
    "Read more at https://en.wikipedia.org/wiki/Skewness\n",
    "\n",
    "<b> Z-Distribution </b>\n",
    "A probability density function and especially a normal distribution that has a mean equal to zero and a standard deviation equal to one and that is used especially in testing hypotheses about means or proportions of samples drawn from populations whose population standard deviations are known "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
